# -*- coding: utf-8 -*-
"""transformer_ì„ì‹œ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cy32yLSDIk3SVSW6s6wOg9oi-L4wZnCQ
"""


# ìµœì¢… í†µí•© ì½”ë“œ (Colab friendly: í•™ìŠµ + ì‹œê°í™”)

#ì°¸ê³ ì½”ë“œ https://github.com/datnnt1997/multi-head_self-attention/blob/master/SelfAttention.ipynb
#ì´ë¡  ì°¸ì¡° ê¹ƒí—ˆë¸Œ https://github.com/IAAR-Shanghai/Awesome-Attention-Heads


import torch                      # PyTorch í•µì‹¬ ëª¨ë“ˆ
import torch.nn as nn             # ì‹ ê²½ë§ êµ¬ì„±ìš© ëª¨ë“ˆ
import torch.nn.functional as F  # í™œì„±í•¨ìˆ˜ ë“±
import pandas as pd              # í‘œ í˜•íƒœ ì¶œë ¥ì„ ìœ„í•œ ëª¨ë“ˆ
from transformers import GPT2Tokenizer  # í† í°í™”ë¥¼ ìœ„í•œ HuggingFace GPT2 tokenizer
from torch.utils.data import DataLoader, Dataset  # ë°ì´í„°ì…‹ ê´€ë¦¬
import random

# GPU ì„¤ì • (ê°€ëŠ¥í•˜ë©´ CUDA ì‚¬ìš©)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ì–´í…ì…˜ ê³„ì‚° í•¨ìˆ˜: scaled dot-product attention
def scaled_dot_product(q, k, v, mask=None):
    d_k = q.size(-1)
    scores = torch.matmul(q, k.transpose(-2, -1)) / d_k**0.5  # ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚° í›„ scaling
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    attn = F.softmax(scores, dim=-1)  # attention ê°€ì¤‘ì¹˜
    output = torch.matmul(attn, v)    # ê°€ì¤‘í•© ê²°ê³¼ ì¶œë ¥
    return output, attn

# ë©€í‹°í—¤ë“œ ì…€í”„ ì–´í…ì…˜ í´ë˜ìŠ¤ ì •ì˜
class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        assert embed_dim % num_heads == 0  # head ìˆ˜ê°€ ë‚˜ëˆ„ì–´ ë–¨ì–´ì ¸ì•¼ í•¨
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        # Q, K, Vë¥¼ ìœ„í•œ ì„ í˜•ë³€í™˜
        self.q_linear = nn.Linear(embed_dim, embed_dim)
        self.k_linear = nn.Linear(embed_dim, embed_dim)
        self.v_linear = nn.Linear(embed_dim, embed_dim)
        self.fc_out = nn.Linear(embed_dim, embed_dim)  # ì¶œë ¥ ê²°í•©

    def forward(self, x):
        B, T, C = x.size()  # ë°°ì¹˜í¬ê¸°, ì‹œí€€ìŠ¤ê¸¸ì´, ì„ë² ë”© ì°¨ì›
        # Q, K, V ìƒì„± í›„ head ì°¨ì› ì¶”ê°€
        Q = self.q_linear(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.k_linear(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.v_linear(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        # ì–´í…ì…˜ ìˆ˜í–‰
        out, attn = scaled_dot_product(Q, K, V)
        # ë‹¤ì‹œ ì›ë˜ ì°¨ì›ìœ¼ë¡œ ë³µì›
        out = out.transpose(1, 2).contiguous().view(B, T, C)
        return self.fc_out(out), attn

# Transformer ìŠ¤íƒ€ì¼ì˜ ì‘ì€ ì–¸ì–´ëª¨ë¸
class TinyTransformer(nn.Module):
    def __init__(self, vocab_size, embed_dim=64, num_heads=4):
        super().__init__()
        self.token_emb = nn.Embedding(vocab_size, embed_dim)  # ë‹¨ì–´ ì„ë² ë”©
        self.attn = MultiHeadSelfAttention(embed_dim, num_heads)  # ì…€í”„ ì–´í…ì…˜
        self.norm = nn.LayerNorm(embed_dim)   #ì •ê·œí™”
        self.lm_head = nn.Linear(embed_dim, vocab_size)  # ì¶œë ¥ ì˜ˆì¸¡ (language modeling head)

    def forward(self, x):
        x = self.token_emb(x)  # [B, T, C]
        x_norm = self.norm(x)               # ì •ê·œí™”
        x_attn, attn = self.attn(x_norm)    # ì–´í…ì…˜ ê²°ê³¼ì™€ ê°€ì¤‘ì¹˜
        x = x + x_attn                      #  Residual ì—°ê²°
        logits = self.lm_head(x)  # ë‹¤ìŒ í† í° ì˜ˆì¸¡

        return logits, attn

from datasets import load_dataset

# TinyStories ë°ì´í„°ì…‹ ì§ì ‘ ë¶ˆëŸ¬ì˜¤ê¸° (ìƒ˜í”Œ 50000ê°œë§Œ ì‚¬ìš©)
dataset = load_dataset("roneneldan/TinyStories", split="train[:50000]")
# í…ìŠ¤íŠ¸ ì¶”ì¶œ
text = "\n".join(example["text"] for example in dataset)




# GPT2 í† í¬ë‚˜ì´ì € ì‚¬ìš©
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token  # pad token ì„¤ì • (eosë¡œ ëŒ€ì²´)
# í…ìŠ¤íŠ¸ë¥¼ token IDë¡œ ì¸ì½”ë”©
encodings = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=64)
input_ids = encodings["input_ids"]  # [num_sentences, seq_len]

'''
from collections import defaultdict
from torch.nn.utils.rnn import pad_sequence

# merge í•¨ìˆ˜: (a,b) ìŒì„ new_indexë¡œ ë¬¶ê¸°
def merge(indices, pair, new_index):
    merged = []
    i = 0
    while i < len(indices):
        if i < len(indices) - 1 and (indices[i], indices[i+1]) == pair:
            merged.append(new_index)
            i += 2
        else:
            merged.append(indices[i])
            i += 1
    return merged

# BPE í•™ìŠµ í•¨ìˆ˜
def train_bpe(text: str, num_merges: int):
    indices = list(text.encode("utf-8"))
    merges = {}
    vocab = {i: bytes([i]) for i in range(256)}

    for i in range(num_merges):
        counts = defaultdict(int)
        for a, b in zip(indices, indices[1:]):
            counts[(a, b)] += 1
        if not counts:
            break
        pair = max(counts, key=counts.get)
        new_index = 256 + i
        merges[pair] = new_index
        vocab[new_index] = vocab[pair[0]] + vocab[pair[1]]
        indices = merge(indices, pair, new_index)

    return {"vocab": vocab, "merges": merges}

# BPE í† í¬ë‚˜ì´ì € í´ë˜ìŠ¤
class BPETokenizer:
    def __init__(self, vocab, merges):
        self.vocab = vocab            # {int: bytes} í˜•íƒœ
        self.merges = merges          # {(a,b): new_index}
        self.inverse_vocab = {v: k for k, v in vocab.items()}  # bytes -> int ì—­ë§¤í•‘

    def encode(self, text: str):
        ids = list(text.encode("utf-8"))
        for (a, b), new_id in self.merges.items():
            ids = merge(ids, (a, b), new_id)
        return ids

    def decode(self, ids: list[int]):
        return b"".join([self.vocab[i] for i in ids]).decode("utf-8", errors="ignore")

# -----------------------
# ì—¬ê¸°ë¶€í„° GPT2Tokenizer ëŒ€ì²´ ë¶€ë¶„

# 1) BPE í•™ìŠµ (num_mergesëŠ” ì ì ˆíˆ ì¡°ì ˆ)
tokenizer_params = train_bpe(text, num_merges=1000)

# 2) í† í¬ë‚˜ì´ì € ê°ì²´ ìƒì„±
tokenizer = BPETokenizer(**tokenizer_params)

# 3) í…ìŠ¤íŠ¸ ì¤„ ë‹¨ìœ„ë¡œ í† í¬ë‚˜ì´ì§• ë° í…ì„œí™”
lines = text.split("\n")
tokenized_ids = [torch.tensor(tokenizer.encode(line)) for line in lines]

# 4) íŒ¨ë”© (íŒ¨ë”© í† í° IDëŠ” 0ìœ¼ë¡œ ì§€ì •)
input_ids = pad_sequence(tokenized_ids, batch_first=True, padding_value=0)  # shape: [num_sentences, seq_len]

# ì´ì œ input_idsë¥¼ ê¸°ì¡´ GPT2Tokenizer ëŒ€ì²´í•´ì„œ í•™ìŠµì— ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥
'''

# PyTorch Dataset ì •ì˜
class SimpleDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return self.data.size(0)

    def __getitem__(self, idx):
        x = self.data[idx, :-1]  # ì…ë ¥: ì• n-1ê°œ
        y = self.data[idx, 1:]   # ì •ë‹µ: ë‹¤ìŒ í† í°
        return x, y

# DataLoader ìƒì„±
dataset = SimpleDataset(input_ids)
dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

# ëª¨ë¸ ì´ˆê¸°í™” ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •
vocab_size = tokenizer.vocab_size
model = TinyTransformer(vocab_size).to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)

# ê°„ë‹¨í•œ í•™ìŠµ ë£¨í”„ (3 epoch)
model.train()
for epoch in range(3):
    total_loss = 0
    for x, y in dataloader:
        x, y = x.to(device), y.to(device)
        logits, _ = model(x)  # ì˜ˆì¸¡
        loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))  # next token loss
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}: loss = {total_loss / len(dataloader):.4f}")

# ë°ëª¨ ì…ë ¥ ë¬¸ì¥ìœ¼ë¡œ attention í™•ì¸
model.eval()
demo_text1 = "Israel England Syria Iran" #ì´ê³³ì— ë‹¨ì–´ ì…ë ¥ ---------------------------------------------->
demo_tokens = tokenizer.tokenize(demo_text1)  # í† í° ë¬¸ìì—´
# í† í¬ë‚˜ì´ì§• ë° í…ì„œí™”
demo_inputs = tokenizer(demo_text1, return_tensors="pt").to(device)
with torch.no_grad():
    _, demo_attn = model(demo_inputs["input_ids"])  # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì¶”ì¶œ

# attention ê°€ì¤‘ì¹˜ í‘œ ì¶œë ¥ í•¨ìˆ˜ ì •ì˜
def print_attention_table(attn_weights, tokens, head=0, batch=0):
    attn = attn_weights[batch, head].detach().cpu().numpy()  # numpy ë³€í™˜
    df = pd.DataFrame(attn, index=tokens, columns=tokens)  # pandas í‘œ
    print(f"\nğŸ”¹ Attention Head {head} Weights:")
    print(df.round(2))  # ì†Œìˆ˜ì  2ìë¦¬

# ë°ëª¨ ì…ë ¥ì— ëŒ€í•œ Head 0ì˜ ì–´í…ì…˜ í‘œ ì¶œë ¥
print_attention_table(demo_attn, demo_tokens, head=0)

demo_text2 = "Israel England France Iran" #ì´ê³³ì— ë‹¨ì–´ ì…ë ¥ --------------------------------------------->
demo_tokens = tokenizer.tokenize(demo_text2)  # í† í° ë¬¸ìì—´
# í† í¬ë‚˜ì´ì§• ë° í…ì„œí™”
demo_inputs = tokenizer(demo_text2, return_tensors="pt").to(device)
with torch.no_grad():
    _, demo_attn = model(demo_inputs["input_ids"])  # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì¶”ì¶œ

# attention ê°€ì¤‘ì¹˜ í‘œ ì¶œë ¥ í•¨ìˆ˜ ì •ì˜
def print_attention_table(attn_weights, tokens, head=0, batch=0):
    attn = attn_weights[batch, head].detach().cpu().numpy()  # numpy ë³€í™˜
    df = pd.DataFrame(attn, index=tokens, columns=tokens)  # pandas í‘œ
    print(f"\nğŸ”¹ Attention Head {head} Weights:")
    print(df.round(2))  # ì†Œìˆ˜ì  2ìë¦¬

# ë°ëª¨ ì…ë ¥ì— ëŒ€í•œ Head 0ì˜ ì–´í…ì…˜ í‘œ ì¶œë ¥
print_attention_table(demo_attn, demo_tokens, head=0)