#### 현재 진행상황
 attention구현 완료 (참고 저장소:https://github.com/datnnt1997/multi-head_self-attention/blob/master/SelfAttention.ipynb)

 
 forward,normalization,add 구현 완료

 
 tokenizer 구현중 (현재는 GPT2 토크나이저 사용중, 밑 각주 참고)

 
 코드 함수 형식은 assignment1과 다름 (기능은 같음)

 
 decoy effect 테스트를 위한 단어간 관계성 계산을 보는 것이 목적이라 attention후 처리해서 출력하는 과정은 없고 바로 단어에 대한 집중도만 표로 표현한다 
 
