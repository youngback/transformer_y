# -*- coding: utf-8 -*-
"""transformer_ì„ì‹œ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cy32yLSDIk3SVSW6s6wOg9oi-L4wZnCQ
"""


# ìµœì¢… í†µí•© ì½”ë“œ (Colab friendly: í•™ìŠµ + ì‹œê°í™”)

#ì°¸ê³ ì½”ë“œ https://github.com/datnnt1997/multi-head_self-attention/blob/master/SelfAttention.ipynb
#ì´ë¡  ì°¸ì¡° ê¹ƒí—ˆë¸Œ https://github.com/IAAR-Shanghai/Awesome-Attention-Heads


import torch                      # PyTorch í•µì‹¬ ëª¨ë“ˆ
import torch.nn as nn             # ì‹ ê²½ë§ êµ¬ì„±ìš© ëª¨ë“ˆ
import torch.nn.functional as F  # í™œì„±í•¨ìˆ˜ ë“±
import pandas as pd              # í‘œ í˜•íƒœ ì¶œë ¥ì„ ìœ„í•œ ëª¨ë“ˆ
from transformers import GPT2Tokenizer  # í† í°í™”ë¥¼ ìœ„í•œ HuggingFace GPT2 tokenizer
from torch.utils.data import DataLoader, Dataset  # ë°ì´í„°ì…‹ ê´€ë¦¬
import random

# GPU ì„¤ì • (ê°€ëŠ¥í•˜ë©´ CUDA ì‚¬ìš©)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ì–´í…ì…˜ ê³„ì‚° í•¨ìˆ˜: scaled dot-product attention
def scaled_dot_product(q, k, v, mask=None):
    d_k = q.size(-1)
    scores = torch.matmul(q, k.transpose(-2, -1)) / d_k**0.5  # ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚° í›„ scaling
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    attn = F.softmax(scores, dim=-1)  # attention ê°€ì¤‘ì¹˜
    output = torch.matmul(attn, v)    # ê°€ì¤‘í•© ê²°ê³¼ ì¶œë ¥
    return output, attn

# ë©€í‹°í—¤ë“œ ì…€í”„ ì–´í…ì…˜ í´ë˜ìŠ¤ ì •ì˜
class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        assert embed_dim % num_heads == 0  # head ìˆ˜ê°€ ë‚˜ëˆ„ì–´ ë–¨ì–´ì ¸ì•¼ í•¨
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        # Q, K, Vë¥¼ ìœ„í•œ ì„ í˜•ë³€í™˜
        self.q_linear = nn.Linear(embed_dim, embed_dim)
        self.k_linear = nn.Linear(embed_dim, embed_dim)
        self.v_linear = nn.Linear(embed_dim, embed_dim)
        self.fc_out = nn.Linear(embed_dim, embed_dim)  # ì¶œë ¥ ê²°í•©

    def forward(self, x):
        B, T, C = x.size()  # ë°°ì¹˜í¬ê¸°, ì‹œí€€ìŠ¤ê¸¸ì´, ì„ë² ë”© ì°¨ì›
        # Q, K, V ìƒì„± í›„ head ì°¨ì› ì¶”ê°€
        Q = self.q_linear(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.k_linear(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.v_linear(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)
        # ì–´í…ì…˜ ìˆ˜í–‰
        out, attn = scaled_dot_product(Q, K, V)
        # ë‹¤ì‹œ ì›ë˜ ì°¨ì›ìœ¼ë¡œ ë³µì›
        out = out.transpose(1, 2).contiguous().view(B, T, C)
        return self.fc_out(out), attn

# Transformer ìŠ¤íƒ€ì¼ì˜ ì‘ì€ ì–¸ì–´ëª¨ë¸
class TinyTransformer(nn.Module):
    def __init__(self, vocab_size, embed_dim=64, num_heads=4):
        super().__init__()
        self.token_emb = nn.Embedding(vocab_size, embed_dim)  # ë‹¨ì–´ ì„ë² ë”©
        self.attn = MultiHeadSelfAttention(embed_dim, num_heads)  # ì…€í”„ ì–´í…ì…˜
        #self.norm = nn.LayerNorm(embed_dim)   #ì •ê·œí™”
        self.lm_head = nn.Linear(embed_dim, vocab_size)  # ì¶œë ¥ ì˜ˆì¸¡ (language modeling head)

    def forward(self, x):
        x = self.token_emb(x)  # [B, T, C]
        x, attn = self.attn(x)  # ì •ê·œí™” XëŠ” ì´ì¤„ í™œì„±í™” í›„ ë°‘ 3ì¤„, ìœ„ì— ì •ê·œí™” íŒŒíŠ¸ ì£¼ì„ì²˜ë¦¬
        #x_norm = self.norm(x)               # ì •ê·œí™”
        #x_attn, attn = self.attn(x_norm)    # ì–´í…ì…˜ ê²°ê³¼ì™€ ê°€ì¤‘ì¹˜
        #x = x + x_attn                      #  Residual ì—°ê²°
        logits = self.lm_head(x)  # ë‹¤ìŒ í† í° ì˜ˆì¸¡
   

        return logits, attn

from datasets import load_dataset

# TinyStories ë°ì´í„°ì…‹ ì§ì ‘ ë¶ˆëŸ¬ì˜¤ê¸° (ìƒ˜í”Œ 50000ê°œë§Œ ì‚¬ìš©)
dataset_full = load_dataset("roneneldan/TinyStories", split="train[:500]")
# 2íšŒì, ì¬ë‹¤ìš´ë¡œë“œ ë°©ì§€
# ì•ì—ì„œ 5000ê°œë§Œ ë½‘ê¸°
dataset = dataset_full.select(range(500))
# í…ìŠ¤íŠ¸ ì¶”ì¶œ
text = "\n".join(example["text"] for example in dataset)




# GPT2 í† í¬ë‚˜ì´ì € ì‚¬ìš©
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token  # pad token ì„¤ì • (eosë¡œ ëŒ€ì²´)
# í…ìŠ¤íŠ¸ë¥¼ token IDë¡œ ì¸ì½”ë”©
encodings = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=64)
input_ids = encodings["input_ids"]  # [num_sentences, seq_len]

'''
import re
from collections import defaultdict
from torch.nn.utils.rnn import pad_sequence

# ê°„ë‹¨í•œ pre-tokenization: ê³µë°± + íŠ¹ìˆ˜ë¬¸ì ê¸°ì¤€ìœ¼ë¡œ í† í° ë¶„ë¦¬
def simple_pre_tokenize(text: str) -> list[str]:
    return re.findall(r"\w+|[^\w\s]", text, re.UNICODE)

# merge í•¨ìˆ˜: (a,b) ìŒì„ new_indexë¡œ ë¬¶ê¸°
def merge(indices, pair, new_index):
    merged = []
    i = 0
    while i < len(indices):
        if i < len(indices) - 1 and (indices[i], indices[i+1]) == pair:
            merged.append(new_index)
            i += 2
        else:
            merged.append(indices[i])
            i += 1
    return merged

# BPE í•™ìŠµ í•¨ìˆ˜
def train_bpe(text: str, num_merges: int):
    indices = list(text.encode("utf-8"))
    merges = {}
    vocab = {i: bytes([i]) for i in range(256)}

    for i in range(num_merges):
        counts = defaultdict(int)
        for a, b in zip(indices, indices[1:]):
            counts[(a, b)] += 1
        if not counts:
            break
        pair = max(counts, key=counts.get)
        new_index = 256 + i
        merges[pair] = new_index
        vocab[new_index] = vocab[pair[0]] + vocab[pair[1]]
        indices = merge(indices, pair, new_index)

    return {"vocab": vocab, "merges": merges}

# ê°œì„ ëœ BPE í† í¬ë‚˜ì´ì €
class BPETokenizer:
    def __init__(self, vocab, merges):
        self.vocab = vocab            # {int: bytes}
        self.merges = merges          # {(a,b): new_index}
        self.inverse_vocab = {v: k for k, v in vocab.items()}  # bytes -> int

    def encode(self, text: str) -> list[int]:
        tokens = simple_pre_tokenize(text)  # ê³µë°±, êµ¬ë‘ì  ë“± ê¸°ì¤€ìœ¼ë¡œ í† í°í™”
        ids = []
        for token in tokens:
            byte_ids = list(token.encode("utf-8"))
            for (a, b), new_id in self.merges.items():
                byte_ids = merge(byte_ids, (a, b), new_id)
            ids.extend(byte_ids)
        return ids

    def decode(self, ids: list[int]) -> str:
        return b"".join([self.vocab[i] for i in ids]).decode("utf-8", errors="ignore")
'''

# PyTorch Dataset ì •ì˜
class SimpleDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return self.data.size(0)

    def __getitem__(self, idx):
        x = self.data[idx, :-1]  # ì…ë ¥: ì• n-1ê°œ
        y = self.data[idx, 1:]   # ì •ë‹µ: ë‹¤ìŒ í† í°
        return x, y

# DataLoader ìƒì„±
dataset = SimpleDataset(input_ids)
dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

# ëª¨ë¸ ì´ˆê¸°í™” ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •
vocab_size = tokenizer.vocab_size
model = TinyTransformer(vocab_size).to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)

# ê°„ë‹¨í•œ í•™ìŠµ ë£¨í”„ (3 epoch)
model.train()
for epoch in range(10):
    total_loss = 0
    for x, y in dataloader:
        x, y = x.to(device), y.to(device)
        logits, _ = model(x)  # ì˜ˆì¸¡
        loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))  # next token loss
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}: loss = {total_loss / len(dataloader):.4f}")

# ë°ëª¨ ì…ë ¥ ë¬¸ì¥ìœ¼ë¡œ attention í™•ì¸
model.eval()
demo_text1 = "Israel England Syria Iran" #ì´ê³³ì— ë‹¨ì–´ ì…ë ¥ ---------------------------------------------->
demo_tokens = tokenizer.tokenize(demo_text1)  # í† í° ë¬¸ìì—´
# í† í¬ë‚˜ì´ì§• ë° í…ì„œí™”
demo_inputs = tokenizer(demo_text1, return_tensors="pt").to(device)
with torch.no_grad():
    _, demo_attn = model(demo_inputs["input_ids"])  # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì¶”ì¶œ

# attention ê°€ì¤‘ì¹˜ í‘œ ì¶œë ¥ í•¨ìˆ˜ ì •ì˜
def print_attention_avg(attn_weights, tokens, batch=0):
    attn = attn_weights[batch].mean(dim=0).detach().cpu().numpy()
    df = pd.DataFrame(attn, index=tokens, columns=tokens)
    print(f"\nğŸ”¹ Attention Heads Average Weights:")
    print(df.round(2))

def print_attention_all_heads(attn_weights, tokens, batch=0):
    num_heads = attn_weights.shape[1]
    for h in range(num_heads):
        attn = attn_weights[batch, h].detach().cpu().numpy()  # Head hì— ëŒ€í•œ attention matrix
        df = pd.DataFrame(attn, index=tokens, columns=tokens)
        print(f"\nğŸ”¹ Head {h} Attention Weights:")
        print(df.round(2))

def print_attention_focus_avg(attn_weights, tokens, batch=0):
    """
    ì–´í…ì…˜ ê°€ì¤‘ì¹˜ì—ì„œ batch=0ì˜ í‰ê·  í—¤ë“œ ê¸°ì¤€ìœ¼ë¡œ
    ì²« ë²ˆì§¸ í† í°ì´ ë‚˜ë¨¸ì§€ 3ê°œ í† í°(1~3)ì— ì–¼ë§ˆë‚˜ ì§‘ì¤‘í•˜ëŠ”ì§€ë¥¼ ë°±ë¶„ìœ¨ë¡œ ì¶œë ¥
    """
    attn = attn_weights[batch].mean(dim=0).detach().cpu().numpy()  # Head í‰ê· 
    values = attn[0, 1:]  # ì²« ë²ˆì§¸ í† í°ì´ 1~3ë²ˆì§¸ì— ì£¼ëŠ” ê°€ì¤‘ì¹˜
    total = values.sum()
    percentages = (values / total) * 100

    df = pd.DataFrame([percentages], columns=tokens[1:], index=[f"{tokens[0]} â†’"])
    print(f"\nğŸ”¹ Head í‰ê·  ê¸°ì¤€ '{tokens[0]}' â†’ ë‚˜ë¨¸ì§€ 3ê°œ í† í° ì§‘ì¤‘ë„ (%):")
    print(df.round(1))

# ë°ëª¨ ì…ë ¥ì— ëŒ€í•œ Head í‰ê· ì˜ ì–´í…ì…˜ í‘œ ì¶œë ¥
print_attention_avg(demo_attn, demo_tokens, batch=0)

print_attention_focus_avg(demo_attn, demo_tokens)

demo_text2 = "Israel England France Iran" #ì´ê³³ì— ë‹¨ì–´ ì…ë ¥ --------------------------------------------->
demo_tokens = tokenizer.tokenize(demo_text2)  # í† í° ë¬¸ìì—´
# í† í¬ë‚˜ì´ì§• ë° í…ì„œí™”
demo_inputs = tokenizer(demo_text2, return_tensors="pt").to(device)
with torch.no_grad():
    _, demo_attn = model(demo_inputs["input_ids"])  # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì¶”ì¶œ

# ë°ëª¨ ì…ë ¥ì— ëŒ€í•œ Head í‰ê· ì˜ ì–´í…ì…˜ í‘œ ì¶œë ¥
print_attention_avg(demo_attn, demo_tokens, batch=0)

print_attention_focus_avg(demo_attn, demo_tokens)
