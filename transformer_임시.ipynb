{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -U datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSWQeqB5PF6K",
        "outputId": "6beb3d4e-4681-47c1-86e3-c9bf2d48a624"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNRddPTXybSd",
        "outputId": "5ab76021-a29c-496d-de41-d7856f04de75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: loss = 10.8119\n",
            "Epoch 2: loss = 10.7987\n",
            "Epoch 3: loss = 10.7854\n",
            "\n",
            "🔹 Attention Head 0 Weights:\n",
            "          Israel  ĠEngland  ĠSyria  ĠIran\n",
            "Israel      0.23      0.24    0.21   0.32\n",
            "ĠEngland    0.24      0.30    0.25   0.21\n",
            "ĠSyria      0.29      0.26    0.21   0.23\n",
            "ĠIran       0.26      0.24    0.20   0.30\n",
            "\n",
            "🔹 Attention Head 0 Weights:\n",
            "          Israel  ĠEngland  ĠFrance  ĠIran\n",
            "Israel      0.21      0.22     0.28   0.29\n",
            "ĠEngland    0.22      0.28     0.30   0.20\n",
            "ĠFrance     0.26      0.22     0.23   0.29\n",
            "ĠIran       0.22      0.19     0.35   0.25\n"
          ]
        }
      ],
      "source": [
        "# 최종 통합 코드 (Colab friendly: 학습 + 시각화)\n",
        "\n",
        "\n",
        "import torch                      # PyTorch 핵심 모듈\n",
        "import torch.nn as nn             # 신경망 구성용 모듈\n",
        "import torch.nn.functional as F  # 활성함수 등\n",
        "import pandas as pd              # 표 형태 출력을 위한 모듈\n",
        "from transformers import GPT2Tokenizer  # 토큰화를 위한 HuggingFace GPT2 tokenizer\n",
        "from torch.utils.data import DataLoader, Dataset  # 데이터셋 관리\n",
        "import random\n",
        "\n",
        "# GPU 설정 (가능하면 CUDA 사용)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 어텐션 계산 함수: scaled dot-product attention\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size(-1)\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) / d_k**0.5  # 유사도 점수 계산 후 scaling\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    attn = F.softmax(scores, dim=-1)  # attention 가중치\n",
        "    output = torch.matmul(attn, v)    # 가중합 결과 출력\n",
        "    return output, attn\n",
        "\n",
        "# 멀티헤드 셀프 어텐션 클래스 정의\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0  # head 수가 나누어 떨어져야 함\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        # Q, K, V를 위한 선형변환\n",
        "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.fc_out = nn.Linear(embed_dim, embed_dim)  # 출력 결합\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()  # 배치크기, 시퀀스길이, 임베딩 차원\n",
        "        # Q, K, V 생성 후 head 차원 추가\n",
        "        Q = self.q_linear(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = self.k_linear(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v_linear(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        # 어텐션 수행\n",
        "        out, attn = scaled_dot_product(Q, K, V)\n",
        "        # 다시 원래 차원으로 복원\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.fc_out(out), attn\n",
        "\n",
        "# Transformer 스타일의 작은 언어모델\n",
        "class TinyTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=64, num_heads=4):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, embed_dim)  # 단어 임베딩\n",
        "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads)  # 셀프 어텐션\n",
        "        self.lm_head = nn.Linear(embed_dim, vocab_size)  # 출력 예측 (language modeling head)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.token_emb(x)  # [B, T, C]\n",
        "        x, attn = self.attn(x)  # 어텐션 결과와 가중치\n",
        "        logits = self.lm_head(x)  # 다음 토큰 예측\n",
        "        return logits, attn\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# TinyStories 데이터셋 직접 불러오기 (샘플 50000개만 사용)\n",
        "dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[:50000]\")\n",
        "# 텍스트 추출\n",
        "text = \"\\n\".join(example[\"text\"] for example in dataset)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# GPT2 토크나이저 사용\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # pad token 설정 (eos로 대체)\n",
        "# 텍스트를 token ID로 인코딩\n",
        "encodings = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
        "input_ids = encodings[\"input_ids\"]  # [num_sentences, seq_len]\n",
        "\n",
        "# PyTorch Dataset 정의\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx, :-1]  # 입력: 앞 n-1개\n",
        "        y = self.data[idx, 1:]   # 정답: 다음 토큰\n",
        "        return x, y\n",
        "\n",
        "# DataLoader 생성\n",
        "dataset = SimpleDataset(input_ids)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# 모델 초기화 및 옵티마이저 설정\n",
        "vocab_size = tokenizer.vocab_size\n",
        "model = TinyTransformer(vocab_size).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
        "\n",
        "# 간단한 학습 루프 (3 epoch)\n",
        "model.train()\n",
        "for epoch in range(3):\n",
        "    total_loss = 0\n",
        "    for x, y in dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits, _ = model(x)  # 예측\n",
        "        loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))  # next token loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}: loss = {total_loss / len(dataloader):.4f}\")\n",
        "\n",
        "# 데모 입력 문장으로 attention 확인\n",
        "model.eval()\n",
        "demo_text1 = \"Israel England Syria Iran\" #이곳에 단어 입력 ---------------------------------------------->\n",
        "demo_tokens = tokenizer.tokenize(demo_text1)  # 토큰 문자열\n",
        "# 토크나이징 및 텐서화\n",
        "demo_inputs = tokenizer(demo_text1, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "    _, demo_attn = model(demo_inputs[\"input_ids\"])  # 어텐션 가중치 추출\n",
        "\n",
        "# attention 가중치 표 출력 함수 정의\n",
        "def print_attention_table(attn_weights, tokens, head=0, batch=0):\n",
        "    attn = attn_weights[batch, head].detach().cpu().numpy()  # numpy 변환\n",
        "    df = pd.DataFrame(attn, index=tokens, columns=tokens)  # pandas 표\n",
        "    print(f\"\\n🔹 Attention Head {head} Weights:\")\n",
        "    print(df.round(2))  # 소수점 2자리\n",
        "\n",
        "# 데모 입력에 대한 Head 0의 어텐션 표 출력\n",
        "print_attention_table(demo_attn, demo_tokens, head=0)\n",
        "\n",
        "demo_text2 = \"Israel England France Iran\" #이곳에 단어 입력 --------------------------------------------->\n",
        "demo_tokens = tokenizer.tokenize(demo_text2)  # 토큰 문자열\n",
        "# 토크나이징 및 텐서화\n",
        "demo_inputs = tokenizer(demo_text2, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "    _, demo_attn = model(demo_inputs[\"input_ids\"])  # 어텐션 가중치 추출\n",
        "\n",
        "# attention 가중치 표 출력 함수 정의\n",
        "def print_attention_table(attn_weights, tokens, head=0, batch=0):\n",
        "    attn = attn_weights[batch, head].detach().cpu().numpy()  # numpy 변환\n",
        "    df = pd.DataFrame(attn, index=tokens, columns=tokens)  # pandas 표\n",
        "    print(f\"\\n🔹 Attention Head {head} Weights:\")\n",
        "    print(df.round(2))  # 소수점 2자리\n",
        "\n",
        "# 데모 입력에 대한 Head 0의 어텐션 표 출력\n",
        "print_attention_table(demo_attn, demo_tokens, head=0)\n"
      ]
    }
  ]
}