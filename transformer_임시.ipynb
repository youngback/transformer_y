{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -U datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSWQeqB5PF6K",
        "outputId": "6beb3d4e-4681-47c1-86e3-c9bf2d48a624"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNRddPTXybSd",
        "outputId": "5ab76021-a29c-496d-de41-d7856f04de75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: loss = 10.8119\n",
            "Epoch 2: loss = 10.7987\n",
            "Epoch 3: loss = 10.7854\n",
            "\n",
            "ğŸ”¹ Attention Head 0 Weights:\n",
            "          Israel  Ä England  Ä Syria  Ä Iran\n",
            "Israel      0.23      0.24    0.21   0.32\n",
            "Ä England    0.24      0.30    0.25   0.21\n",
            "Ä Syria      0.29      0.26    0.21   0.23\n",
            "Ä Iran       0.26      0.24    0.20   0.30\n",
            "\n",
            "ğŸ”¹ Attention Head 0 Weights:\n",
            "          Israel  Ä England  Ä France  Ä Iran\n",
            "Israel      0.21      0.22     0.28   0.29\n",
            "Ä England    0.22      0.28     0.30   0.20\n",
            "Ä France     0.26      0.22     0.23   0.29\n",
            "Ä Iran       0.22      0.19     0.35   0.25\n"
          ]
        }
      ],
      "source": [
        "# ìµœì¢… í†µí•© ì½”ë“œ (Colab friendly: í•™ìŠµ + ì‹œê°í™”)\n",
        "\n",
        "\n",
        "import torch                      # PyTorch í•µì‹¬ ëª¨ë“ˆ\n",
        "import torch.nn as nn             # ì‹ ê²½ë§ êµ¬ì„±ìš© ëª¨ë“ˆ\n",
        "import torch.nn.functional as F  # í™œì„±í•¨ìˆ˜ ë“±\n",
        "import pandas as pd              # í‘œ í˜•íƒœ ì¶œë ¥ì„ ìœ„í•œ ëª¨ë“ˆ\n",
        "from transformers import GPT2Tokenizer  # í† í°í™”ë¥¼ ìœ„í•œ HuggingFace GPT2 tokenizer\n",
        "from torch.utils.data import DataLoader, Dataset  # ë°ì´í„°ì…‹ ê´€ë¦¬\n",
        "import random\n",
        "\n",
        "# GPU ì„¤ì • (ê°€ëŠ¥í•˜ë©´ CUDA ì‚¬ìš©)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ì–´í…ì…˜ ê³„ì‚° í•¨ìˆ˜: scaled dot-product attention\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size(-1)\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1)) / d_k**0.5  # ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚° í›„ scaling\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    attn = F.softmax(scores, dim=-1)  # attention ê°€ì¤‘ì¹˜\n",
        "    output = torch.matmul(attn, v)    # ê°€ì¤‘í•© ê²°ê³¼ ì¶œë ¥\n",
        "    return output, attn\n",
        "\n",
        "# ë©€í‹°í—¤ë“œ ì…€í”„ ì–´í…ì…˜ í´ë˜ìŠ¤ ì •ì˜\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0  # head ìˆ˜ê°€ ë‚˜ëˆ„ì–´ ë–¨ì–´ì ¸ì•¼ í•¨\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        # Q, K, Vë¥¼ ìœ„í•œ ì„ í˜•ë³€í™˜\n",
        "        self.q_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_linear = nn.Linear(embed_dim, embed_dim)\n",
        "        self.fc_out = nn.Linear(embed_dim, embed_dim)  # ì¶œë ¥ ê²°í•©\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()  # ë°°ì¹˜í¬ê¸°, ì‹œí€€ìŠ¤ê¸¸ì´, ì„ë² ë”© ì°¨ì›\n",
        "        # Q, K, V ìƒì„± í›„ head ì°¨ì› ì¶”ê°€\n",
        "        Q = self.q_linear(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = self.k_linear(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v_linear(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        # ì–´í…ì…˜ ìˆ˜í–‰\n",
        "        out, attn = scaled_dot_product(Q, K, V)\n",
        "        # ë‹¤ì‹œ ì›ë˜ ì°¨ì›ìœ¼ë¡œ ë³µì›\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.fc_out(out), attn\n",
        "\n",
        "# Transformer ìŠ¤íƒ€ì¼ì˜ ì‘ì€ ì–¸ì–´ëª¨ë¸\n",
        "class TinyTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=64, num_heads=4):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, embed_dim)  # ë‹¨ì–´ ì„ë² ë”©\n",
        "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads)  # ì…€í”„ ì–´í…ì…˜\n",
        "        self.lm_head = nn.Linear(embed_dim, vocab_size)  # ì¶œë ¥ ì˜ˆì¸¡ (language modeling head)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.token_emb(x)  # [B, T, C]\n",
        "        x, attn = self.attn(x)  # ì–´í…ì…˜ ê²°ê³¼ì™€ ê°€ì¤‘ì¹˜\n",
        "        logits = self.lm_head(x)  # ë‹¤ìŒ í† í° ì˜ˆì¸¡\n",
        "        return logits, attn\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# TinyStories ë°ì´í„°ì…‹ ì§ì ‘ ë¶ˆëŸ¬ì˜¤ê¸° (ìƒ˜í”Œ 50000ê°œë§Œ ì‚¬ìš©)\n",
        "dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[:50000]\")\n",
        "# í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
        "text = \"\\n\".join(example[\"text\"] for example in dataset)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# GPT2 í† í¬ë‚˜ì´ì € ì‚¬ìš©\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # pad token ì„¤ì • (eosë¡œ ëŒ€ì²´)\n",
        "# í…ìŠ¤íŠ¸ë¥¼ token IDë¡œ ì¸ì½”ë”©\n",
        "encodings = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
        "input_ids = encodings[\"input_ids\"]  # [num_sentences, seq_len]\n",
        "\n",
        "# PyTorch Dataset ì •ì˜\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx, :-1]  # ì…ë ¥: ì• n-1ê°œ\n",
        "        y = self.data[idx, 1:]   # ì •ë‹µ: ë‹¤ìŒ í† í°\n",
        "        return x, y\n",
        "\n",
        "# DataLoader ìƒì„±\n",
        "dataset = SimpleDataset(input_ids)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# ëª¨ë¸ ì´ˆê¸°í™” ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
        "vocab_size = tokenizer.vocab_size\n",
        "model = TinyTransformer(vocab_size).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
        "\n",
        "# ê°„ë‹¨í•œ í•™ìŠµ ë£¨í”„ (3 epoch)\n",
        "model.train()\n",
        "for epoch in range(3):\n",
        "    total_loss = 0\n",
        "    for x, y in dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits, _ = model(x)  # ì˜ˆì¸¡\n",
        "        loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))  # next token loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}: loss = {total_loss / len(dataloader):.4f}\")\n",
        "\n",
        "# ë°ëª¨ ì…ë ¥ ë¬¸ì¥ìœ¼ë¡œ attention í™•ì¸\n",
        "model.eval()\n",
        "demo_text1 = \"Israel England Syria Iran\" #ì´ê³³ì— ë‹¨ì–´ ì…ë ¥ ---------------------------------------------->\n",
        "demo_tokens = tokenizer.tokenize(demo_text1)  # í† í° ë¬¸ìì—´\n",
        "# í† í¬ë‚˜ì´ì§• ë° í…ì„œí™”\n",
        "demo_inputs = tokenizer(demo_text1, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "    _, demo_attn = model(demo_inputs[\"input_ids\"])  # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì¶”ì¶œ\n",
        "\n",
        "# attention ê°€ì¤‘ì¹˜ í‘œ ì¶œë ¥ í•¨ìˆ˜ ì •ì˜\n",
        "def print_attention_table(attn_weights, tokens, head=0, batch=0):\n",
        "    attn = attn_weights[batch, head].detach().cpu().numpy()  # numpy ë³€í™˜\n",
        "    df = pd.DataFrame(attn, index=tokens, columns=tokens)  # pandas í‘œ\n",
        "    print(f\"\\nğŸ”¹ Attention Head {head} Weights:\")\n",
        "    print(df.round(2))  # ì†Œìˆ˜ì  2ìë¦¬\n",
        "\n",
        "# ë°ëª¨ ì…ë ¥ì— ëŒ€í•œ Head 0ì˜ ì–´í…ì…˜ í‘œ ì¶œë ¥\n",
        "print_attention_table(demo_attn, demo_tokens, head=0)\n",
        "\n",
        "demo_text2 = \"Israel England France Iran\" #ì´ê³³ì— ë‹¨ì–´ ì…ë ¥ --------------------------------------------->\n",
        "demo_tokens = tokenizer.tokenize(demo_text2)  # í† í° ë¬¸ìì—´\n",
        "# í† í¬ë‚˜ì´ì§• ë° í…ì„œí™”\n",
        "demo_inputs = tokenizer(demo_text2, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "    _, demo_attn = model(demo_inputs[\"input_ids\"])  # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì¶”ì¶œ\n",
        "\n",
        "# attention ê°€ì¤‘ì¹˜ í‘œ ì¶œë ¥ í•¨ìˆ˜ ì •ì˜\n",
        "def print_attention_table(attn_weights, tokens, head=0, batch=0):\n",
        "    attn = attn_weights[batch, head].detach().cpu().numpy()  # numpy ë³€í™˜\n",
        "    df = pd.DataFrame(attn, index=tokens, columns=tokens)  # pandas í‘œ\n",
        "    print(f\"\\nğŸ”¹ Attention Head {head} Weights:\")\n",
        "    print(df.round(2))  # ì†Œìˆ˜ì  2ìë¦¬\n",
        "\n",
        "# ë°ëª¨ ì…ë ¥ì— ëŒ€í•œ Head 0ì˜ ì–´í…ì…˜ í‘œ ì¶œë ¥\n",
        "print_attention_table(demo_attn, demo_tokens, head=0)\n"
      ]
    }
  ]
}